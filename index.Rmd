---
output:
  html_document:
    css: styles.css
    self_contained: TRUE
    highlight: tango
    code_folding: hide
    toc: TRUE
    toc_float: TRUE
---

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans|Roboto" />

<a href="http://www.trafforddatalab.io" target="_blank"><img src="images/logo.png" style="height:50px;position:absolute;top:15px;left:25px;" border="0"/></a>

<br />
<br />
<br />

## Open Data companion
Last updated: 11 July 2018

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, prompt = FALSE, tidy = FALSE, comment = NA, message = FALSE, warning = FALSE)
library(knitr)
```

Governments and other organisations often make [open data](http://5stardata.info/en/) available through Web service Application Programming Interfaces or APIs. The [World Bank](https://datahelpdesk.worldbank.org/knowledgebase/topics/125589), [UK Police](https://data.police.uk/docs/), and [Transport for London](https://api.tfl.gov.uk/) are just a few well-known examples. This document details the steps required to request data from these different Web service APIs using [R](https://cran.r-project.org/). 

Several R packages[^1] have been developed as clients for Web service APIs. These don't assume any knowledge of API endpoints, HTTP requests, or data formats like XML and JSON. These are really convenient but sometimes you want to break into the 'black box' of APIs because you want to learn more or because there isn't an API wrapper package available.

### A quick introduction to APIs

APIs or Application Programming Interfaces are a set of rules that allow one software application to interact with another either in the same location or over a network. Inputs and outputs will vary between APIs but the process is the same: a 'request' that follows certain programmatic rules is submitted and a 'response' containing content in an expected format is returned.

There are many types of API including library-based (e.g. leafletJS) and class-based (e.g. Java) but one of the most common are Web service APIs. A client (browser) submits a Hypertext Transfer Protocol (HTTP) request to a server and the server returns a response to the client. The response contains status information about the request and may also contain the requested content. 

The parameters of an HTTP request are typically contained in the URL. For example, to return a map image of Manchester using the [Google Maps Static API](https://developers.google.com/maps/documentation/maps-static/dev-guide) we would submit the following request:

[https://maps.googleapis.com/maps/api/staticmap?center=Manchester,England&zoom=13&size=600x300&maptype=roadmap](https://maps.googleapis.com/maps/api/staticmap?center=Manchester,England&zoom=13&size=600x300&maptype=roadmap)

The request contains:

1) a URL to the API endpoint (https\://maps.googleapis.com/maps/api/staticmap?) and;
2) a query containing the parameters of the request (center=Manchester,England&zoom=13&size=600x300&maptype=roadmap). In this case, we have specified the location, zoom level, size and type of map.

Web service APIs use two key HTTP verbs to enable data requests:  **GET** and **POST**. A GET request is submitted within the URL with each parameter separated by an ampersand (`&`). A POST request is submitted in the message body which is separate from the URL. The advantage of using POST over GET requests is that there are no character limits and the request is more secure because it is not stored in the browser's cache.

There are several types of Web service APIs (e.g. XML-RPC, JSON-RPC and SOAP) but the most popular is Representational State Transfer or REST. RESTful APIs can return output as XML, JSON, CSV and several other data formats. 

Each API has documentation and specifications which determine how data can be transferred. Unfortunately, the specifications tend to be different and the documentation can be hard to follow.

### An example API request
Querying a Web service API typically involves the following steps:

1) submit the request
2) check for any server error
3) parse the response
4) convert to a data frame

In the following example we will submit a request for police reported crime data from the [UK Police](https://www.police.uk/) API. The API uses both HTTP GET and POST requests and provides content in JSON data format.

The two key R packages for submitting HTTP requests to Web service APIs and parsing the content of the response are [httr](https://cran.r-project.org/web/packages/httr/index.html) and [jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html). Let's load them into our R session.
```{r, }
library(tidyverse) ; library(httr) ; library(jsonlite)
```

We would like to retrieve street level crimes within a mile radius of a specific location so according to the UK Police [API documentation](https://data.police.uk/docs/) we need to use https\://data.police.uk/api/crimes-street/all-crime? as our API endpoint. Rather than 'all-crime' lets narrow our request to retrieve only reports of burglary. This will change our path to: https\://data.police.uk/api/crimes-street/burglary?

```{r}
path <- "https://data.police.uk/api/crimes-street/burglary?"
```

Next we need to build our API request and submit it. We will use the `GET` function from the httr package. First we supply the path to the API endpoint and provide search parameters in the form of a list to the `query` argument. There are three parameters available to us:

- `lat` = latitude
- `lng` = longitude
- `date` = and optional date in YYYY-MM format

```{r}
request <- GET(url = path, 
               query = list(
                 lat = 53.421813,
                 lng = -2.330251,
                 date = "2017-01")
               )
```

Next we parse the content returned from the server as text using the `content` function.

```{r}
response <- content(request, as = "text", encoding = "UTF-8")
```

Let's check if the API returned an error. If the request fails the API will return a non-200 status code.
```{r}
if(request$status_code != 200) stop(response)
```

Lastly we'll parse the content and and convert it to a data frame.
```{r}
df <- fromJSON(response, flatten = TRUE) %>% 
  data.frame()
```

```{r, echo = FALSE}
kable(df %>% select(7,1,11,8,9) %>% slice(1:4), caption = "Burglaries within 1m radius of specified location")
```
 
## Setup

The following API requests all rely on the [tidyverse](https://cran.r-project.org/web/packages/tidyverse/index.html), [httr](https://cran.r-project.org/web/packages/httr/index.html), and [jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html) R packages. If you have not installed the packages please do so and then load them into your R session.

```{r}
# install.packages("tidyverse", "httr", "jsonlite")
library(tidyverse) ; library(httr) ; library(jsonlite)
```

<br />

## Food Standards Agency

The [Food Standards Agency](http://ratings.food.gov.uk/) provide food hygiene rating data for the United Kingdom.

***Example***: fast food outlets in Trafford

- **HTTP verb**: GET
- **API endpoint URL**: [http://api.ratings.food.gov.uk/Establishments](http://api.ratings.food.gov.uk/Establishments)
- **Selected parameters**: name, address, longitude, latitude, businessTypeId, ratingKey, localAuthorityId etc.
- **Header(s)**: "x-api-version", 2
- **Data format(s)**: JSON, XML
- **Documentation**: [http://api.ratings.food.gov.uk/help](http://api.ratings.food.gov.uk/help)

```{r}
# submit the request
request <- GET(url = "http://api.ratings.food.gov.uk/Establishments", 
             query = list(
               localAuthorityId = 188,
               BusinessTypeId = 7844,
               pageNumber = 1,
               pageSize = 5000),
             add_headers("x-api-version" = "2"))

# parse the response
response <- content(request, as = "text", encoding = "UTF-8") %>% 
  fromJSON(flatten = TRUE) %>% 
  pluck("establishments") %>% 
  as_tibble()

# tidy the data
df <- response %>% 
  mutate_all(funs(replace(., . == '', NA))) %>% 
  select(name = BusinessName,
         type = BusinessType,
         address1 = AddressLine1,
         address2 = AddressLine2,
         address3 = AddressLine3,
         address4 = AddressLine4,
         postcode = PostCode,
         long = geocode.longitude,
         lat = geocode.latitude) %>% 
  unite(address, address1, address2, address3, address4, remove = TRUE, sep = ", ") %>% 
  mutate(address = str_replace_all(address, "NA,", ""),
         address = str_replace_all(address, ", NA", ""),
         long = as.numeric(long),
         lat = as.numeric(lat))
```

```{r, echo = FALSE}
kable(df %>% slice(1:6), caption = "Fast food outlets in Trafford")
```

<br />

## Nomis

[Nomis](https://www.nomisweb.co.uk/) provide labour market, benefit and census data for the United Kingdom. 

***Example***: Claimant count data

- **HTTP verb**: GET
- **API endpoint URL**: https://www.nomisweb.co.uk/api/v01/dataset/
- **Selected parameters**: 
- **Header(s)**: NA
- **Data format(s)**: CSV
- **Documentation**: [https://www.nomisweb.co.uk/api/v01/help](https://www.nomisweb.co.uk/api/v01/help)
- **R package**: [nomisr](https://cran.r-project.org/web/packages/nomisr/index.html)

First, let's load the necessary R packages.

```{r}
library(tidyverse) ; library(httr) ; library(jsonlite) ; library(stringr)
```

Then we need to pull the metadata for all of the Nomis datasets that are available to query via the API.

```{r}
metadata <- fromJSON("https://www.nomisweb.co.uk/api/v01/dataset/def.sdmx.json", flatten = TRUE) %>%
  map("keyfamilies") %>%
  map_df(bind_rows) %>% 
  unnest(components.dimension) %>% 
  select(id, name = name.value, description = description.value, parameter = conceptref)
```

We'd like to obtain the id for the [Claimant count by sex and age](https://www.nomisweb.co.uk/datasets/ucjsa) dataset so let's list all datasets with the keywords 'claimant' and 'age' using a regular expression.

```{r}
filter(metadata, str_detect(name, regex('^.*?\\b(claimant(s)*)\\b.*?\\bage\\b.*?$', ignore_case = T))) %>%
  distinct(id, name) %>% 
  kable(caption = "Nomis datasets with 'claimant' and 'age' keywords")
```

We are interested in 'Claimant count by sex and age' which has the id of 'NM_162_1'. We can therefore update the API URL with the dataset id: http://www.nomisweb.co.uk/api/v01/dataset/NM_162_1

Next we need to identify the search parameters (or dimensions) available for NM_162_1. Each dataset can have a number of different parameters such as 'date', 'geography', 'gender', and 'measures'. The date parameter can have values like 'latest', 'prevyear' etc. All the other available parameters and their corresponding values need to be identified from the dataset's metadata.

```{r}
filter(metadata, id == "NM_162_1") %>% 
  pull(parameter) %>% 
  kable(col.names = "Parameter", caption = "Available parameters for NM_162_1")
```

To find which genders are available for the ‘Claimant count by sex and age’ dataset we need to add `gender` to the API URL and `def.sdmx.json` which extracts the codelist for the dataset: https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1/gender/def.sdmx.json 

```{r}
fromJSON("https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1/gender/def.sdmx.json", flatten = TRUE) %>% 
  as.data.frame() %>% 
  unnest() %>%
  select(description = description.value, value) %>% 
  kable(caption = "Available categories of gender")
```

We want 'Total' which has a value of 0. To append this to the API URL we would add `&gender=0`. The ampersand is used to separate each parameter in the query string of the API URL.

To find which geographic areas the claimant count data are available at requires a slight tweak to the API URL to the dataset codelist: https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1/geography/TYPE/def.sdmx.json

```{r}
fromJSON("https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1/geography/TYPE/def.sdmx.json", flatten = TRUE) %>% 
  as.data.frame() %>% 
  unnest() %>%
  select(description = description.value, value) %>% 
  head() %>% 
  kable(caption = "First six available geographic areas")
```

We'd like claimant count data at the '2011 super output areas - lower layer' level so we'd append `&geography=TYPE298` to the API URL

All the parameters we require and the corresponding values of interest are listed below:

* date=latest (latest available period)
* geography=TYPE298 (2011 super output areas - lower layer)
* gender=0 (Total)
* age=0 (All categories: Age 16+)
* measure=1 (Claimant count)
* measures=20100 (value)

We can now append all the necessary parameters to the API URL Note that we have added `.data.csv` to the dataset id which is required to pull CSV data from the API. 

https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1.data.csv?date=latest&geography=TYPE298&gender=0&age=0&measure=1&measures=20100

```{r}
df <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_162_1.data.csv?date=latest&geography=TYPE298&gender=0&age=0&measure=1&measures=20100") 

kable(select(df, 1:6) %>% slice(1:6), caption = "Raw claimant count data")
```

Then we can tidy the results up a little by filtering by Greater Manchester boroughs, creating new variables and renaming others.

```{r}
df %>% 
  filter(grepl('Bolton|Bury|Manchester|Oldham|Rochdale|Salford|Stockport|Tameside|Trafford|Wigan', GEOGRAPHY_NAME)) %>% 
  mutate(date = as.Date(paste('01', DATE_NAME), format = '%d %B %Y'),
         lad17nm = str_sub(GEOGRAPHY_NAME, 1, str_length(GEOGRAPHY_NAME)-5),
         measure = "Residents claiming JSA or Universal Credit") %>% 
  select(date,
         lsoa11nm = GEOGRAPHY_NAME,
         lsoa11cd = GEOGRAPHY_CODE, 
         lad17nm,
         measure,
         value = OBS_VALUE) %>% 
  head() %>% 
  kable(caption = "Tidy claimant count data")
```

<br />

## UK Police

The [data.police.uk](https://data.police.uk/) website provides incidents of police recorded crime and anti-social behaviour in England, Wales and Northern Ireland.

***Example***: Crime and ASB data within the borough of Trafford.

- **HTTP verb**: GET or POST
- **API endpoint URL**: https://data.police.uk/api/crimes-street/all-crime?
- **Selected parameters**: poly, date
- **Header(s)**: NA
- **Data format(s)**: JSON
- **Documentation**: [https://data.police.uk/docs/](https://data.police.uk/docs/)


```{r}
# load the necessary R packages
library(tidyverse) ; library(httr) ; library(jsonlite) ; library(sf)

# download a vector boundary of Trafford
bdy <- st_read("https://opendata.arcgis.com/datasets/fab4feab211c4899b602ecfbfbc420a3_3.geojson", quiet = TRUE) %>% 
  filter(lad17nm == "Trafford")

# extract the coordinates and format for inclusion in the API request parameter
coords <- bdy %>% 
  st_coordinates() %>%
  as.data.frame() %>%
  select(X, Y) %>% 
  unite(coords, Y, X, sep = ',') %>% 
  mutate(coords = sub("$", ":", coords)) %>% 
  .[["coords"]] %>% 
  paste(collapse = "") %>% 
  str_sub(., 1, str_length(.)-1)

# sumbit the API request
request <- POST(url = "https://data.police.uk/api/crimes-street/all-crime",
                query = list(poly = coords, date = "2018-04"))

# process the content
content <- content(request, as = "text", encoding = "UTF-8")

# parse the response
results <- fromJSON(txt = content, flatten = TRUE) 

# convert to a data frame
df <- data.frame(
  month = results$month,
  category = results$category,
  location = results$location.street.name,
  long = as.numeric(as.character(results$location.longitude)),
  lat = as.numeric(as.character(results$location.latitude)),
  stringsAsFactors = FALSE
)
```

```{r, echo = FALSE}
head(df) %>% kable(caption = "Crime and ASB data for Trafford, April 2018")
```

<br />

[^1]: Examples include [eurostat](https://cran.r-project.org/web/packages/eurostat/index.html), [fingertipsR](https://cran.r-project.org/web/packages/fingertipsR/index.html), and [WHO](https://cran.r-project.org/web/packages/WHO/index.html)